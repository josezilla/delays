{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13 - Starter Code\n",
    "\n",
    "Spacy Demo\n",
    "\n",
    "If you haven't installed spacy yet, use:\n",
    "```\n",
    "conda install spacy\n",
    "python -m spacy.en.download\n",
    "```\n",
    "This downloads about 500 MB of data.\n",
    "\n",
    "Another popular package, `nltk`, can be installed as follows (you can skip this for now):\n",
    "\n",
    "```\n",
    "conda install nltk\n",
    "python -m nltk.downloader all\n",
    "```\n",
    "\n",
    "This also downloads a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load StumbleUpon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"../datasets/stumbleupon.tsv\", sep='\\t',\n",
    "                  encoding=\"utf-8\")\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load spacy\n",
    "\n",
    "from spacy.en import English\n",
    "nlp_toolkit = English()\n",
    "nlp_toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to load `spacy`:\n",
    "```\n",
    "import spacy\n",
    "nlp_toolkit = spacy.load(\"en\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = u\"IBM sees holographic calls, air breathing batteries\"\n",
    "parsed = nlp_toolkit(title)\n",
    "\n",
    "for (i, word) in enumerate(parsed): \n",
    "    print \"Word: {}\".format(word)\n",
    "    print \"\\t Phrase type: {}\".format(word.dep_)\n",
    "    print \"\\t Is the word a known entity type? {}\".format(\n",
    "        word.ent_type_  if word.ent_type_ else \"No\")\n",
    "    print \"\\t Lemma: {}\".format(word.lemma_)\n",
    "    print \"\\t Parent of this word: {}\".format(word.head.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For your infos:\n",
    "#### .dep_: (Syntactic Dependency Parsing)\n",
    "\n",
    "Alphabetical listing\n",
    "\n",
    "- acl: clausal modifier of noun (adjectival clause)\n",
    "- advcl: adverbial clause modifier\n",
    "- advmod: adverbial modifier\n",
    "- amod: adjectival modifier\n",
    "- appos: appositional modifier\n",
    "- aux: auxiliary\n",
    "- case: case marking\n",
    "- cc: coordinating conjunction\n",
    "- ccomp: clausal complement\n",
    "- clf: classifier\n",
    "- compound: compound\n",
    "- conj: conjunct\n",
    "- cop: copula\n",
    "- csubj: clausal subject\n",
    "- dep: unspecified dependency\n",
    "- det: determiner\n",
    "- discourse: discourse element\n",
    "- dislocated: dislocated elements\n",
    "- expl: expletive\n",
    "- fixed: fixed multiword expression\n",
    "- flat: flat multiword expression\n",
    "- goeswith: goes with\n",
    "- iobj: indirect object\n",
    "- list: list\n",
    "- mark: marker\n",
    "- nmod: nominal modifier\n",
    "- nsubj: nominal subject\n",
    "- nummod: numeric modifier\n",
    "- obj: object\n",
    "- obl: oblique nominal\n",
    "- orphan: orphan\n",
    "- parataxis: parataxis\n",
    "- punct: punctuation\n",
    "- reparandum: overridden disfluency\n",
    "- root: root\n",
    "- vocative: vocative\n",
    "- xcomp: open clausal complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ent_type: \n",
    "\n",
    "Named entity type?\n",
    "\n",
    "\n",
    "### head:\n",
    "\n",
    "The syntactic parent, or \"governor\", of this token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate Page Titles\n",
    "\n",
    "Let's see if we can find organizations in our page titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def references_organization(title):\n",
    "    parsed = nlp_toolkit(title)\n",
    "    return any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "\n",
    "data['references_organization'] = data['title'].fillna(u'').map(references_organization)\n",
    "\n",
    "# Take a look\n",
    "data[data['references_organization']][['title']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Write a function to identify titles that mention an organization (ORG) and a person (PERSON).\n",
    "\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise solution\n",
    "\n",
    "def references_org_person(title):\n",
    "    parsed = nlp_toolkit(title)\n",
    "    contains_org = any([word.ent_type_ == 'ORG' for word in parsed])\n",
    "    contains_person = any([word.ent_type_ == 'PERSON' for word in parsed])\n",
    "    return contains_org and contains_person\n",
    "\n",
    "data['references_org_person'] = data['title'].fillna(u'').map(references_org_person)\n",
    "\n",
    "# Take a look\n",
    "data[data['references_org_person']][['title']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting \"Greenness\" Of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset comes from [stumbleupon](https://www.stumbleupon.com/), a web page recommender.  \n",
    "\n",
    "A description of the columns is below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FieldName|Type|Description\n",
    "---------|----|-----------\n",
    "url|string|Url of the webpage to be classified\n",
    "title|string|Title of the article\n",
    "body|string|Body text of article\n",
    "urlid|integer| StumbleUpon's unique identifier for each url\n",
    "boilerplate|json|Boilerplate text\n",
    "alchemy_category|string|Alchemy category (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "alchemy_category_score|double|Alchemy category score (per the publicly available Alchemy API found at www.alchemyapi.com)\n",
    "avglinksize| double|Average number of words in each link\n",
    "commonlinkratio_1|double|# of links sharing at least 1 word with 1 other links / # of links\n",
    "commonlinkratio_2|double|# of links sharing at least 1 word with 2 other links / # of links\n",
    "commonlinkratio_3|double|# of links sharing at least 1 word with 3 other links / # of links\n",
    "commonlinkratio_4|double|# of links sharing at least 1 word with 4 other links / # of links\n",
    "compression_ratio|double|Compression achieved on this page via gzip (measure of redundancy)\n",
    "embed_ratio|double|Count of number of <embed> usage\n",
    "frameBased|integer (0 or 1)|A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "frameTagRatio|double|Ratio of iframe markups over total number of markups\n",
    "hasDomainLink|integer (0 or 1)|True (1) if it contains an <a> with an url with domain\n",
    "html_ratio|double|Ratio of tags vs text in the page\n",
    "image_ratio|double|Ratio of <img> tags vs text in the page\n",
    "is_news|integer (0 or 1) | True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "lengthyLinkDomain| integer (0 or 1)|True (1) if at least 3 <a> 's text contains more than 30 alphanumeric characters\n",
    "linkwordscore|double|Percentage of words on the page that are in hyperlink's text\n",
    "news_front_page| integer (0 or 1)|True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "non_markup_alphanum_characters|integer| Page's text's number of alphanumeric characters\n",
    "numberOfLinks|integer Number of <a>|markups\n",
    "numwords_in_url| double|Number of words in url\n",
    "parametrizedLinkRatio|double|A link is parametrized if it's url contains parameters or has an attached onClick event\n",
    "spelling_errors_ratio|double|Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "label|integer (0 or 1)|User-determined label. Either evergreen (1) or non-evergreen (0); available for train.tsv only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Let's try extracting some of the text content.\n",
    "> ### Create a feature for the title containing 'recipe'. Is the % of evegreen websites higher or lower on pages that have recipe in the the title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['recipe'] = data['title'].str.contains('recipe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Use of the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = data['title'].fillna('')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1000, \n",
    "                             ngram_range=(1, 2), \n",
    "                             stop_words='english',\n",
    "                             binary=True)\n",
    "\n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Demo: Build a random forest model to predict evergreeness of a website using the title features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 20)\n",
    "    \n",
    "# Use `fit` to learn the vocabulary of the titles\n",
    "vectorizer.fit(titles)\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "X = vectorizer.transform(titles).toarray()\n",
    "y = data['label']\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, scoring='roc_auc')\n",
    "print('CV AUC {}, Average AUC {}'.format(scores, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Build a random forest model to predict evergreeness of a website using the title features and quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature\n",
    "# Identify the features you want from the original dataset\n",
    "\n",
    "# Stack them horizontally together\n",
    "# This takes all of the word/n-gram columns and appends on two more columns\n",
    "# Use: from scipy.sparse import hstack\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "\n",
    "# What features of these are most important?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Build a random forest model to predict evergreeness of a website using the body features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "# Use vectorizer.fit() to learn the vocabulary\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Exercise: Use `TfIdfVectorizer` instead of `CountVectorizer` - is this an improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use `fit` to learn the vocabulary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Use `tranform` to generate the sample X word matrix - one column per feature (word or n-grams)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
